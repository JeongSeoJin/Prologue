프롤로그 보고서 (7월 10일 ~ 7월 31일)

라즈베리파이 셋팅(요즘 라즈비안 설치 환경이 바뀜) 편리해짐

 라즈베리파이에 라즈비안(os)를 설치한 후 vnc통신과 ssh 통신을 위해서 설정들을 enable로 전환
특히 이번 프로젝트는 opencv를 사용하는 것이기 때문에 CLI환경에서는 작업하기가 매우 불편하기
도 하고 매번 라즈베리파이에 모니터, 키보드, 마우스 등을 연결해서 하는 것이 번거롭기 때문에
GUI원격 통신을 이용함 (작업하기가 훨씬 수월해짐)

 opencv를 라즈베리파이에 설치함(과정이 매우 복잡)
온전히 설치를 끝낸 후 예전에 내 컴퓨터에서 미리 짜 두었던 코드를 github를 통해 clone함.

 코드의 내용은 웹캠을 통해서 실시간으로 얼굴을 인식하고 (나중에 모듈을 물체 인식 모듈로 바꿀
예정. 현재는 테스트를 위해서) 얼굴의 좌표 값을 추출 후 판단하여 얼굴이 어떠한 위치에 있는지
폰트로 표시해줌

 이것을 라즈베리파이에서 작동할 수 있도록 코드를 수정하고 라즈베리파이용 카메라를 통해 얼굴인식
을 원활히 할 수 있게됨 (얼굴이 인식된 위치와 보여지는 화면의 중점 위치를 비교하여 어떤 방향으로
움직여야 화면의 중심으로 갈 수 있는지 가이드 해주도록 제작)

 조금 더 발전해서 서보모터를 이용한 얼굴 추적 프로그램을 제작함 (MG955서보모터 위에 임시적으로
 라즈베리파이용 카메라를 부착하고 위에서 제작한 코드를 응용하여 인식된 얼굴이 화면의 중심과 멀어질 때
 얼굴을 추적하여 움직이도록 제작) 통신은 RPI.GPIO를 통해서 이루어짐(이것 또한 라이브러리)

 다량의 서보모터를 위해서 pca9685서보 드라이브를 이용하려 시도하였지만, 외부전압의 문제로
현재는 구동이 불가(현재 프로젝트에 필요한 서보모터(MG-955서보모터)는 높은 전압을 요구하기 때문에
비교적 낮은 전압을 요구하는 SG-90서보모터는 pca9685서보 드라이버를 이용하여 통신 가능하도록
구현함)

 stt, tts를 구현하여 우리의 프로젝트에서 사용하고 싶다는 욕심이 들어 긴 시간동안 공부하고 코드를
짠 결과 윈도우 환경에서는 음성인식과, 음성출력 더 나아가 간단한 명령들을 수행할 수 있는 임시
비서 프로그램을 제작해보았고 성공적으로 구현하였다. 음성인식은 SpeechRecognition라이브러리를
사용하였고 음성 출력은 pyttsx3라이브러리(라즈베리파이에서는 pyttsx3를 사용)를 사용하였다.

 하지만 문제는 이 코드를 라즈베리파이에서 구현하는 것이 문제이다. 코드를 짜서 원도우 환경에서
완벽히 구현했다고 하더라도 라즈베리파이의 os는 윈도우와 완전 다른 체계인 linux기반이기 때문에 
보통은 적어도 조금은 수정을 해야한다. 우리의 편의성을 위해 제작된 윈도우, mac os와는 다르게 
개발자를 위한 linux는 하나하나 다 손을 봐줘야하는 부분에서 번거로움이 있고 대부분의 사람들도
이러한 부분에서 오류가 나, 긴 시간 동안 검색과 구글링, stackoverflow를 통해 해결한다.(이러면서
공부함)



[7월 31일]

[하드웨어 부분]
오늘은 로봇 팔의 베이스를 제작 (로봇의 지지대 부분 가장 큰 2개의 축 부분)
3d프린트 출력(지속적으로 3d출력을 하는데 탈조 현상이 생겨서 문제가 됨 => 문제의 정확한 원인은 
찾지 못하였지만 최종적으로는 해결함 내가 생각한 가장 큰 원인은 프린터의 헤드의 속도가 빨랐기 
때문에 프린터가 흔들렸고 이로 인해서 탈조현상이 생겼다고 판단, 보통의 경우로는 프린터의 벨트가
느슨해져서 헛도는 경우가 생겨 탈조현상이 생길 수 있음)

프린트를 출력하고 난 후 간단한 가공과정을 거치고 조립(출력물의 크기가 컸기 때문에 부품을 나눠서 출력함)

[소프트웨어 부분]
어제 결국 구현해내지 못한 sst, tts를 라즈베리파이에서 성공적으로 구현(하지만 윈도우는 음성을 지원
(win32com.client)해주는 반면에 라즈베리파이는 기본 음성을 사용해야했고 변경이 불가하기 때문에 
처음에는 굉장히 기괴하였지만 여러가지 방법을 찾아본결과 속도 조절을 가능케 하여 부자연스럽지만
전보다는 자연스러워짐)

전에 stt, tts가 잘 구현되지 않았던 이유는 마이크는 있었지만 스피커가 없어서 테스트하기에 조금 더
불리했었던 조건이 있었기 때문이고, 내가 코드에서 입력은 가능하게 코드를 정상적으로 작성했었지만 
입력을 받아서 출력까지 해줘야하는 코드에 문제가 있었기 때문이다. 코드를 발견하고 즉시 수정하여
테스트 해본 결과 성공적으로 구현함

**음성인식 기술 : 보통 특정한 단어나 짧은 문장을 입력 받아야만 인식이 가능하지만 나는 이러한 부분에서
조금 말하는 것에 있어서 제한적이라고 느껴 문장 전체를 입력받은 후 그 문장에서 특정한 단어가 포함되어 있다면 
다음 코드를 실행 할 수 있도록 제작해 조금 더 자유롭게 말해도 인식이 가능하도록 코드를 제작했다
**

이후 음성을 통해서 서보모터 뿐만 아니라 다른 기능적인 것들을 컨트롤 하고 싶어서 전에 제작해 두었던 서보모터를
제어하는 코드를 가져와 이 코드에 접목시켰고, 'rotate my servo'라는 말이 입력받은 문장에 있다면 각도를 입력할
수 있도록 재물음 하여 성공적으로 서보모터를 음성인식으로 제어할 수 있게 됨


[8월 3일]

[하드웨어, 소프트웨어 부분]
어쩌다 보니 소프트웨어 부분과 하드웨어 부분을 동시에 진행하고 있는 중인데 오늘은 저번에 해결하지 못한 다중 서보
제어를 성공시켜보고자 다짐했다. 저번에 다중서보 제어가 원활히 구동되어지지 않았던 점을 외부 전원의 부족으로 
분석했었는데 오늘 아버지께서 외부 전원 공급장치를 빌려주셔서 내가 생각했었던 문제가 맞는지 확인해 볼 수 있었다.
pca9685모듈에 외부 전원 장치를 연결하고 mg995서보가 요구하는 전압을 넣어 주었더니 정상적으로 작동하는 것을
확인할 수 있었다. 현재까지 가장 큰 문제였던 다중 서보 제어를 해결하고 나니, 현재 할 수 있는 것들이 더욱 더
많아졌다. 

또한 3d모델링과 프린팅도 계속 진행중이여서 방학이 끝나기 전까지는 하드웨어를 완성할 수 있을 것이라고 예상한다.
현재 로봇 팔의 베이스 부분은 거의 완성이 되었고 팔의 중간 부분을 제작 중이다. 하지만 모델링을 할 때 물리적으로
위치, 규격, 사이즈, 등 많은 것들을 계산 한 후 모델링을 해야하는 것이기 때문에 베이스 부분보다 조금 더 복잡한
그래퍼 부분에서는 어떠한 변수들이 있을지 몰라서 사실 고민이다. 


[소프트웨어 부분]
다중 서보모터 제어가 가능해졌으니 실험 삼아 저번에 opencv와 서보모터를 이용하여 제작했던 얼굴 추적프로그램을
조금 수정하여 x,y축을 따라 모두 움직일 수 있는 추적이 가능한 시스템(?)을 제작하였다.
그리고 이 로봇 팔을 음성제어를 통해서 동작 할 수 있도록 제작하여 자동으로 동작하는 것도 좋지만, 수동으로도 
로봇 팔을 제어할 수 있도록 제작하여 팔이 동작하는데 있어서 제한적인 부분을 줄이고자 고안해 낸 방법이다.
로봇 팔은 윈도우뿐만 아니라 linux, mac os와 같은 체제에서도 제공해주는 tkinter(TK GUI툴 킷에 대한 표준
python 인터페이스)를 이용하여 각각의 관절들(서보모터)를 제어할 수 있도록 코드를 짰다.
현재 tkinter를 이용하여 서보모터를 제어할 수 있지만 scale라이브러리를 이용하여 제어할지 아니면 키보트 이벤트의 
값을 입력받아서 사용할지는 아직 고민중에 있다.